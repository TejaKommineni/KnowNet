%%% -*-LaTeX-*-

\chapter{BACKGROUND}

Gaining insights from the network data using Machine Learning(ML) and Data Mining(DM) is a trending research area. There is a lot of confusion in the literature about the terms ML, DM as they often employ the same methods and therefore overlap significantly. Hence, below we describe the process of ML and DM briefly and establish why we have chosen data mining for building our system.

Machine Learning is a method of generating rules from past data to predict the future.
A Machine Learning approach usually consists of two phases:
training and testing. The following steps are usually performed:
\begin{itemize}
	\item Identifying attributes (features) and classes from training data.
	\item Choosing a subset of the attributes for classification.
	\item Choosing an ML algorithm to create a model using the training data.
	\item Use the trained model to classify the unknown data.
\end{itemize}	

Data Mining is the process of extracting patterns and structures implicit in the data and provide them in an understandable format to the user. Data mining is generally considered as a step in the process of Knowledge Discovery from Data (KDD). KDD usually consists of the following steps:
\begin{itemize}
	\item Selection of raw data from which knowledge has to be extracted.
	\item Preprocessing the data to perform cleaning and filtering to avoid noise.
	\item Transforming the data so that all the attributes in the raw data are aligned towards achieving the common goal. 
	\item Applying data mining algorithms to find rules or patterns.
	\item Interpret the observations of the above step and validate them.
	
\end{itemize}

As outlined above though both the techniques have similar steps of preprocessing data they differ on the end goal while ML maps the data set to known classes DM tries to find patterns out of the data set.

The decision of the approach that we want to employ in our problem solving also depends on the type of data we have in hand. If the data is completely labeled, the problem is called supervised learning and generally the task is to find a function or model that explains the data. The approaches such as curve fitting or machine-learning methods are used to model the data to the underlying problem. The label is generally a meaningful tag that is informative and has relation to the collected data.  When a portion of the data is labeled during acquisition of the data or by human experts, the problem is called semi-supervised learning. The addition of the labeled data greatly helps to solve the problem. In general case, the semi-supervised learning problem is converted to supervised learning problem by labeling the whole dataset using the known labeled data and again the same machine-learning methods can be employed here. In unsupervised learning problems, we don't have any labels in the given data set and the main task is to find patterns, structures, or knowledge from this unlabeled data.   

The dataset that we used for addressing our problem is flow data collected at routers which is explained in detail in section \ref{design}. This flow data generally comes without any labels. As explained above if the data in hand doesn't have any labels the problem we are solving is called unsupervised learning problem. Also, in the introduction, we have mentioned that the main goal of this work is to extract host behaviors from the aggregate data. This problem falls under a category of finding implicit/unseen patterns. Thus, having unlabeled data and the problem that we are trying to solve led us to use data mining techniques in building our system.

\section{Unsupervised learning}  \label{unsupervised}
As we are approaching our problem using unsupervised techniques in data mining. Here is a brief explanation of different techniques within unsupervised learning.
Unsupervised learning problems can be further grouped into clustering and association problems. A clustering problem is where you want to discover the inherent groupings in the data, and find patterns. An association rule learning problem is where you want to discover rules that describe large portions of your data, such as given an event X there is a chance of event Y happening. Since we aim to find the inherent groupings our focus will be on clustering techniques. Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Below we describe few of these techniques.

Connectivity models \cite{doi:10.1093/comjnl/26.4.354}, work on the premise that data points close to each other exhibit higher similarity than those lying far away. Cluster formation in these models can be done in two ways. Either we can initially consider all the points as a single cluster and then divide them into multiple clusters based on the distance between data points or we can assume each data point is a cluster on its own and start merging them as the distance decreases. Hierarchical clustering algorithms fall under this category.

Centroid models, also use distance as their metric in grouping data points to a cluster. While connectivity models consider the distance between all points, centroid models measure the distance of a point from cluster centers so as to assign it to the closest center. These models are iterative in nature and come to a halt when the cluster centers don't change. There are different techniques to initially choose cluster centers.  K-Means clustering algorithm \cite{Derpanis06k-meansclustering} is a popular algorithm that falls into this category. K-Center, K-medoids also belong to the same family.
 
Distribution models\cite{Johnson2000}, Datasets can be fit into different distributions (eg: Normal, Gaussian). The clustering models that group all the data points with similar distribution and generate clusters according to these distributions fall under this category. A well-known example of these models is Expectation-maximization algorithm. 

Density Models \cite{thang2011anomaly}, In these models clusters of data points, are determined based on the variation of density in the data space. Different density regions within the data space are found out and all those points that fall under same density region are assigned to the same cluster. Popular examples of density models are DBSCAN and OPTICS.

The choice of clustering model depends on the data in hand, amount of prior information that we have about the data and the problem we ought to solve. In our case, we have chosen centroid models to solve our problem and specifically the K-Means algorithm. Before, discussing why K-Means in design section let us look at what K-Means algorithm is all about.

K-means [17] is a clustering algorithm that groups data points based on their feature values into K disjoint clusters. All the points that are classified into the same cluster are similar to each other when compared to those data points in the other clusters. The number of clusters K has to be specified in advance. Here are the four steps of the K-means clustering
algorithm:

\begin{itemize}
	\item Define the number of clusters K.
	
	\item Initialize the K cluster centers. We can arbitrarily choose K data points from the given data set as cluster centers or apply K-means++ \cite{arthur2007k} algorithm to find the initial K cluster centers. K-means++ helps in careful initial seeding that can output better clusters.
	
	\item Iterate over all data points and compute the distances to the centers of all clusters. Assign each object to the cluster to which it is nearest.
	
	\item Recalculate the cluster centers after the above assignments.
	
	\item Repeat step 3 until the cluster centers do not change anymore.
\end{itemize}

A distance function is required in order to compute the
distance (i.e. similarity) between two objects. The most commonly used distance function is the Euclidean one which is defined as:

\begin{center}
	\boldsymbol{$ d(x,y) = \sqrt{\sum_{i = 1}^{m}(x_i-y_i)^2}$}
\end{center}
where $x = (x_1, ..., x_m)$ and $y = (y_1, ..., y_m)$ are two input vectors(data points) with m features. In the Euclidean distance function, each feature is weighted equally. However, in our data set different features are usually measured at different scales. Hence, they must be normalized before applying the distance function.