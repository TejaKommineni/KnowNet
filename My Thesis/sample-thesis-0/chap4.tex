%%% -*-LaTeX-*-

\chapter{Implementation}

In this chapter we will discuss about the implementation details of our system. 

The raw data collected at the routers is exported to Netflow
collectors as described in section 3. NFDUMP \cite{} tools are
used to process the netflow data. They support netflow version v5,v7
and v9. The two tools of primary importance are nfcapd
(netflow capture daemon) that reads the netflow data from
the network and stores the data into files. nfdump (netflow
dump) reads the netflow data from the files stored by nfcapd.
Using nfdump we import the data on to our local machines
from the Netflow Collectors. This imported data forms the
primary source for our experiments. We collect real network
traffic from a campus network with almost XXX users.

Within Feature Engineering we used different python libraries such as NumPy, Spacy and Pandas to clean and manipulate data. Specifically, the mean and median calculating methods from Pandas for missing data and min max scaler, long transformer from NumPys.
For aggregating all the features by a host we used MongoDB to decrease the amount of time it takes for aggregation by indexing \cite{} and inserting the file in database we were able to reduce this steps time from to 20 mts per day. In this context we also wanted to try the approach of streaming algorithms such as bloom filters or count min sketch which gives the aggregate information with little memory foot print which we have xplored a little but left as future work.

As mentioned we have chosen K-Means for our pattern detector and to determine K we used elbow and silhouette approaches. Here we used scikit-learn package implementations for K-Means 
