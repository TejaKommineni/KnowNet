%%% -*-LaTeX-*-

\chapter{Results}

In this chapter we evaluate our system using an implementation of the NetFlow host behavior extractor written in python. Our experiments in this section are conducted on a ubuntu16 virtual machine with 80 GB memory and 20 cpus of computing power. 

\section{Dataset: description and clustering}
The traffic analyzed here is collected at the University of Utah's emulab routers \cite{White+:osdi02}, This is a private repository that contains traces of bidirectional traffic into the emulab network and no payload. Traffic is collected on Full Duplex links at a speed of 1Gbps.

We passed the NetFlow records collected over the last 6 months of 2017 through our system to determine the set of behaviors(clusters) hosts exhibited over this time. We have hand picked 7 days of this data for analysis. The chosen days are October 15, October 16, November 14, November 23, December 5, December 12, December 16\footnote{Days are picked without any prior knowledge of any metadata related to NetFlow records}. We made sure that there is a mix of both week days , weekends, normal and heavy traffic days. Each days traffic is treated independently as the same host that appeared on two different days could exhibit different behaviors. For different evaluations in this section we used the data corresponding to the these days.

As mentioned earlier in the section \ref{cluster_labeling} we have choosen a reference day for labeling the clusters. We applied following techniques to label the clusters on the reference day. The first one is port-based analysis to identify applications such as web, mail, and DNS. second one is the anomaly detection methods. Finally, a set of heuristic rules to label manually the behavior of the hosts. After the above steps we found the following clusters. Clusters labeled as T consists of hosts which use few ports on both sides to exchange interactive traffic. Clusters labeled as S consists of hosts which serve clients requests and conversely clusters labeled C consists of hosts which request servers for information. Let us list the differences in these behaviors in detail.

\textbf{Clusters T1-T2} are mostly one-to-one connections, the dominant traffic in these clusters is http/https, ssh and peer to peer traffic. P2P traffic is defined as traffic where hosts uses both TCP and UDP ports concurrently for communication, They also choose arbitary ports for communication. Further analysis, reveals that clusters are split based on packet size and flow size.
While T1 has long living flows with large average packet sizes. T2 has large number of flows which are short lived and an average packet size less than T1.

\textbf{Clusters C1-C2} contains hosts which behave as clients  making connections with different servers, The cluster C2 is dominated by DNS traffic which is in accordance with the heavy DNS requests that the hosts residing in the emulab network make with the external servers (machines outside the Emulab Firewall). Cluster C1 on the other hand comprises of hosts that request for web pages and that have outbound mail traffic.

\textbf{Clusters S1-S2} as mentioned above comprises of hosts that behave as servers which respond to client requests. These hosts generally communicate with different ports of destination machines from a fixed port. S1 cluster contains of majority of hosts which are within emulab network and are acting as servers accepting SSH requests. S2 cluster contains hosts that are both within and outside emulab network which are acting as servers for different sets of traffic such as SSH, Web and DNS which is further explained in the section \ref{cross_validation}. 

\textbf{Cluster A1} is the cluster that is of interest for network security as it contains of hosts that exhibit anamolous behavior trying to scan different access points to enter into the system.

The above description clearly points out that host based behavior extraction produces a richer and finer classification of host behaviors than a port based classifier. For example, Http traffic is split across different clusters T or S which represents how the same protocol can be used differently in real time scenarios. Same is the case with SSH traffic also. While most of these clusters contain hosts using different protocols. They are similar in the sense that each cluster exhibits different functional behavior such as Server, Client or One-to-One traffic.

The \figref{behaviorsdec12} is a comparison between the classification made by a traditional port based classifer 6.1(a) and our host based classifier 6.1(b). The traditional port based classifier identified the top 8 applications used by the hosts in the system. DNS(53) is the most used application with 11.95\% of the whole traffic trying to query DNS servers. It is followed by Telnet(23), SSH(22) with a share of 9.83\% and 5.08\% traffic respectively. We can also see web traffic both secured and unsecured in the top 8 accounting to 3.84\%, 0.66\% traffic. While, we can see the applications that accounted to majority of traffic by traditional classifier, Our system on the other hand provided an alternate view for the same NetFlow data by extracting the behaviors these hosts are exhibting. From the \figref{behaviorsdec12} we can see that, hosts that are behaving as clients account to 11.41\% (both C1 and C2 combined) of traffic and hosts that are behaving as servers account to 33.61\% (both S1 and S2 combined). The majority of the traffic(50.23\%) comprises of interactive communication between hosts while the anomalous hosts trying to invade into the network form the minority(4.75\%). 

\begin{figure}[t]
	\centerline{\includegraphics[trim=2cm 2cm 2cm 2cm, scale = 0.5]{dec12-port-behaviors.pdf}}
	\caption{Comparison of hosts using port based classifier (left) and host behavior extractor (right) on December 12, 2017.}%
	\figlabel{behaviorsdec12}
\end{figure}


\section{Cross Validation} \label{cross_validation}
We compared the results obtained from our system with other classification techniques to better understand the significance of our approach.

\textbf{\textit{Cross Validation with a port based classifier}}, We used a classic port based classifying technique to cross validate our results. Though, it isn't a perfect measure and fails in many cases it is good enough to classify a host to a particular class in most of the cases.
In the scenarios when this technique cannot classify a host to particular class we label it as a 'Mix' traffic. The heuristic that we used for labeling as 'Mix' traffic is when the dominant class accounts for less than 50 percent of the traffic by that host. Applying this procedure we observed 20 different classes of traffic, out of which the most frequently observed are discussed here: HTTP, DNS, SSH, MAIL, TELNET, FTP, CHAT, P2P, SMTP, MIX. The cross-validation between the port based approach and  our procedure on one of the choosen day December 12th is reported in \tabref{validation}.

\begin{table}[b]
	\caption{Cross-valdation of the host behavior extraction with port based analysis.}%
	\centerline{\includegraphics[scale = 0.5]{validation.pdf}}	
	\tablabel{validation}
\end{table}

The row headers in the table correspond to the lables of the clusters generated by our system while the column headers correspond to the different classes of traffic derived by a port based classifier as mentioned above. Each row in the table describes the percentage of hosts within each cluster that fall under different classes of traffic as classified by a port based classifier. For example, in the cluster T1 we have 60.88\% of hosts that are using web applications, 0.86\% of hosts are involved in DNS traffic, 15.03\% of hosts exchange SSH, 0.72\% exchange TELNET traffic and 22.04\% of the hosts in this cluster are classified as MIX traffic. Some important observations from this table are:

\begin{itemize}
	\item From the highlited parts in the table we can safely conclude that we have an high match of host classification which reflects the competence of the proposed procedure.
	
	\item  It is also clearly evident from here that the nature of clusters described earlier confirm with the results obtained with cross-validation.
	
	\item This also suggests that all the hosts that fall into a particular class based on port based analysis are not necessarily in the same cluster. They are dispersed across the clusters which shows the necessity of a host based behavior extraction.	
\end{itemize}
  For example, cluster S1 clearly confirms the fact that the hosts in this cluster are dealing with SSH traffic and the same traffic is also present in T1 and T2 clusters. Similary, hosts in the cluster S2 contains of servers that provide responses for web, ssh and other requests. This cluster has similar distribution as cluster C1 but they act exactly opposite like servers and clients respectively. Also, few points of observation from the table are, T1 has mostly requests and responses between hosts spread across different applications. hosts which are grouped as T2 have most of it's hosts labeled as Mix, which on further investigation revealed that many of the hosts in this class don't pass the majority test to be distinguished into any particular class. Finally, Scanners distribution was minimal across all the clusters. The sparsity of this table also serves as a confident measure for our proposed approach.

The following evaluations also validate our systems results.
Our observation is that the behaviors exhibited by the hosts are quite stable over several months which can be seen in the  \figref{constant}. The graph is a representation of the behaviors exhibited by hosts over a six months of time period. Since we map each cluster to a behavior this is equals to the number of clusters that are formed over a time period. As, we are using K-means clustering, in our case this is a graph drawn between the  value of K on X-axis and the day on which we found this value on Y-axis. Except for few points it is a straight line indicating the constants behaviors that hosts exhibited in our system. Our recommendation is to update the host behaviors every couple of months or as per the change in usage of applications.

\begin{figure}[t]
	\centerline{\includegraphics{constant.png}}
	\caption{ Plot of number of clusters formed over a time period.}%
	\figlabel{constant}
\end{figure}


\begin{figure}[t]
	\centerline{\includegraphics[trim=2cm 2cm 2cm 2cm, scale = 0.7]{bytes_cdf.pdf}}
	\caption{ Average Bytes feature CDF for clusters T1,T2,S1,S2}%
	\figlabel{bytes_cdf}
\end{figure}

\figref{bytes_cdf} is a CDF of the average bytes transferrd by hosts in T1,T2 and S1,S2 clusters. From the CDFs of T1 and T2 we can see that the hosts in T1 have an average byte size which is higher than hosts in the cluster T1 and consequenlty this feature turned out to be a distinguishing factor of these two clusters. On the other hand S1 and S2 clusters have a similar average byte size which points out that the feature average byte is not the distinguishing factor of these two clusters and that no single feature had dominance in formation of the clusters.

\section{Synthesized Data: description and clustering}

In the evaluations described in the previous sections of this chapter we used the NetFlow data that doesn't have any ground truth in other terms the hosts in the NetFlow records are not mapped to any particular category of traffic which makes it tough to measure the correctness of our system. This is why we employed port based classification in section 6.2 to validate our system though it is not a correct measure all the times. In this section we have considered a data set that is a mix of real world traffic with synthesized traffic to evaluate our system. This data set is provided by Center for Applied Internet Data Analysis (CAIDA). CAIDA is a research group based at the San Diego Supercomputer Center (SDSC) on the UC San Diego campus who conduct network research and build research infrastructure to support large-scale data collection, curation, and data distribution to the scientific research community. This dataset consists of four captures of different samples. Each scenario(capture) is executed with a goal to evaluate disparate functionalities. Each scenario was captured in a pcap file which was further processed to obtain information about NetFlows. \tabref{synthdesc} represents the duration of each scenario and the contents of the pcap file such as the number of packets, the number of NetFlows and the size of the file itself. 
%explain how caida synthesized data%
\begin{table}[t]
	\caption{Amount of data on each scenario.}%
	\centerline{\includegraphics[scale = 0.5]{synth_desc.pdf}}	
	\tablabel{synthdesc}
\end{table}

The distinctive character of this data set is that each scenario is manually examined and labeled. After labeling the data set looks similar to the one which we capture at the emulab router excpet for the last column where the flow is labeled which can be seen in the \figref{ss_labeled}.

\begin{figure}[b]
	\centerline{\includegraphics[trim=4cm 3cm 3cm 3cm, scale = 0.5]{ss_labeled.pdf}}
	\caption{Manually labeled synthetic data.}%
	\figlabel{ss_labeled}
\end{figure}

As mentioned earlier in this section these four different scenarios contain real network traffic and synthesized data. In the first scenario data is simulated to reflect Scanning web proxies, In the second one traffic patterns exhibited by chinese hosts is simulated, The third and fourth scenarios have a mix of user generated traffic that mimics the behavior of UDP and ICMP DDOS attacks and multiple synchronized networks respectively. Using these different captures(scenarios) we evaluated our systems basic functionality such as how effectively our system is able to extract the host behaviors, the detection rate of the different possible attacks and how will it respond with scale (when there are multiple users on multiple networks communicating simultaneously). The results are as follows:

\subsection{Scenario 1:}
The scenario 1 is of duration 11.63 hours with 37.6 GB of data. This dataset contains hosts exhibiting two unique behaviors namely normal traffic and scanners as labeled by the manual inspection. Through this experiment we would like to check our systems basic functionality of identifying different behaviors. When we passed this data through our sytsem we found formation of two major clusters and the results are as shown in  \tabref{scenario1}. This is a confusion matrix and we can learn the following information from this matrix:

\begin{table}[b]
	\caption{Scenario 1.}%
	\centerline{\includegraphics[scale = 0.5]{scenario1.pdf}}	
	\tablabel{scenario1}
\end{table}

\begin{itemize}
	\item There are two possible predicted behaviors: 'Scanning Web Flows' and 'Normal Flows'. 
	\item Our system made a total of 129,833 predictions.
	\item Out of those our system predicted 46,490 flows are from hosts which are scanning and 83,343 flows are from hosts which are exhibiting Normal behavior.
	\item In reality 84,239 flows are from hosts that show Normal behavior and 45,594 flows are from hosts that are scanners.	
\end{itemize}

Now, let us calculate some numbers that describe the performance of our system.

\begin{itemize}
	\item \textbf{Accuracy:} Overall, how often is our system identifying correctly?
	\begin{itemize}
	
	\item Number of flows it as predicted as scanners and they  actually are flows belonging to scanners summed up with Number of flows it as predicted as Normal and they  actually are flows belonging to Normal hosts divided by the total predictions made by our system.
	
	\item (42503+80252)/129833 = 0.9454
	
	\end{itemize}

	\item \textbf{Misclassification Rate:} Overall, how often is our system wrong?
	\begin{itemize}
		
		\item Sum of number of flows our system has predicted wrong across all the behaviors divided by total predictions made by our system. In this case, number of flows it has predicted as scanners when they are actually flows belonging to Normal hosts summed up with number of flows it has predicted as Normal when they actually belong to Scanning hosts divided by the total predictions made by our system.
		
		\item (3987+3091)/129833 = 0.0545
	\end{itemize}
	
	\item \textbf{Precision:} For a certain behavior, how often it is identifying correctly?
	\begin{itemize}
		
		\item 
		 The number of flows for a certain behavior that are identified correctly divided by total number of flows predicted for that behavior.
		 Number of flows which are correctly predicted as scanners divided by total number of flows predicted as scanners, Number of flows which are correctly predicted as Normal divided by total number of flows predicted as Normal. 
		
		\item (42503)/46490 = 91.4\% precise in finding scanners.
		
		\item (80252)/83343 = 96.2\% precise in finding normal hosts.
			
	\end{itemize}

\end{itemize}


From this we learn that our system can identify unique behaviors with high accuracy and precision.

\subsection{Scenario 2:}
The scenario 2 is of duration 0.38 hours with 5.8 GB of data. This dataset contains hosts exhibiting two unique behaviors namely normal traffic and chinese hosts as labeled by the manual inspection. This experiment is aimed to determine the basic functionality of our system as scenario 1 and to determine the rate of detection of chinese hosts which are prevalent in network attacks in our real data set collected at Emulab routers. When we passed this data through our system we found formation of two major clusters and the results are as shown in  \tabref{scenario2}. This information that we can learn from the confusion matrix is:

\begin{table}[b]
	\caption{Scenario 2.}%
	\centerline{\includegraphics[scale = 0.5]{scenario2.pdf}}	
	\tablabel{scenario2}
\end{table}

\begin{itemize}
	\item There are two possible predicted behaviors: 'Chinese Host Flows' and 'Normal Flows'. 
	\item Our system made a total of 107,978 predictions.
	\item Out of those our system predicted 28,368 flows are from hosts which are hosted in china and 79,610 flows are from hosts which are exhibiting Normal behavior.
	\item In reality 82,625 flows are from hosts that show Normal behavior and 25,353 flows are from hosts that are hosted in chinese.	
\end{itemize}

For this scenario the performance numbers are as follows:

\begin{itemize}
	\item \textbf{Accuracy:} Overall, how often is our system identifying correctly?
	\begin{itemize}
		
		\item Number of flows it as predicted as from hosts hosted in chinese and they  actually are flows belonging to chinese hosts summed up with Number of flows it as predicted as Normal and they  actually are flows belonging to Normal hosts divided by the total predictions made by our system.
		
		\item (22456+76713)/107978 = 0.89184
		
	\end{itemize}
	
	\item \textbf{Misclassification Rate:} Overall, how often is our system wrong?
	\begin{itemize}
		
		\item Number of flows it as predicted as chinese hosts and they are not flows belonging to chinese hosts along with Number of flows it as predicted as Normal and they are not flows belonging to Normal hosts divided by the total predictions made by our system.
		
		\item (5912+2897)/107978 = 0.117
	\end{itemize}
	
	\item \textbf{Precision:} For a certain behavior, how often it is identifying correctly?
	\begin{itemize}
		
		\item Number of flows which are correctly predicted as chinese hosts divided by total number of flows predicted as chinese hosts, Number of flows which are correctly predicted as Normal divided by total number of flows predicted as Normal. 
		
		\item (22456)/28368 = 76.1\% precise in finding chinese hosts.
		
		\item (76713)/79610 = 96.3\% precise in finding normal hosts.
		
	\end{itemize}
	
\end{itemize}




\subsection{Scenario 3:}
The scenario 3 is of duration 4.21 hours with 53 GB of data. This dataset contains hosts exhibiting three unique behaviors namely normal ,background traffic and flows from hosts trying to do a DDOS as labeled by the manual inspection. When we passed this data through our system we found formation of three major clusters and the results are as shown in  \tabref{scenario3}. This information that we can learn from the confusion matrix is:

\begin{table}[b]
	\caption{Scenario 3.}%
	\centerline{\includegraphics[scale = 0.55]{scenario3.pdf}}	
	\tablabel{scenario3}
\end{table}

\begin{itemize}
	\item There are two possible predicted behaviors: 'Background Flows' ,'Normal Flows', 'UDP \& ICMP DDOS Flows'. 
	\item Our system made a total of 1,111,836 predictions.
	\item Out of those our system predicted 689,249 flows are from hosts which are exhibiting Normal behavior , 73,650 flows are from hosts which are trying to perform DDOS attack and 348,937 flows as Background flows.
	\item In reality the flows from normal hosts, background hosts and DDOS attackers are  711459, 332104 and 68273 respectively.
\end{itemize}

For this scenario the performance numbers are as follows:

\begin{itemize}
	\item \textbf{Accuracy:} Overall, how often is our system identifying correctly?
	\begin{itemize}
		
		\item (681167+66377+320011)/1111836 = 0.9601
		
	\end{itemize}
	
	\item \textbf{Misclassification Rate:} Overall, how often is our system wrong?
	\begin{itemize}
		
	
	\end{itemize}
	
	\item \textbf{Precision:} For a certain behavior, how often it is identifying correctly?
	\begin{itemize}
		
		
		
	\end{itemize}
	
\end{itemize}