%%% -*-LaTeX-*-

\chapter{Results}

In this chapter we evaluate our system using an implementation of the NetFlow host behavior extractor written in python. Our implementation uses, Our experiments in this section are conducted on , 

\section{Dataset: description and clustering}
The traffic analyzed here is collected at the University of Utah's emulab routers \cite{}, This is a private repository that contains traces of ? incoming traffic into the emulab network and no payload. Traffic is collected on a link between ? at a speed of ?.

First, the clustering algorithm is applied on aggregated host data collected from the NetFlow records that belongs to the last 6 months in 2017, hence yieding a set of behaviors(clusters). We have hand picked 7 days of this data for analysis. The chosen days are October 15, October 16, November 14, November 23, December 5, December 12, December 16\footnote{Days are picked without any prioir knowledge}. We made sure that there is a mix of both week days , weekends, normal and heavy traffic days. Each days traffic is treated independently as the same host that appeared on two different days could exhibit different behaviors.

As mentioned in the section \ref{cluster_labeling} we have choosen a reference day for labeling the clusters. We applied following techniques to label the clusters on the reference day. The first one is port-based analysis to identify applications such as web, mail, and DNS. second one is the anomaly detection methods. Finally, a set of heuristic rules to label manually the behavior of the hosts. After the above steps we found the following clusters. Clusters labeled as T consists of hosts which uses few ports on both sides to exchange interactive traffic. Clusters labeled as S consists of hosts which serve clients requests and conversely clusters labeled C consists of hosts which request servers for information. Let us list the differences in these behaviors in detail.

\textbf{Clusters T1-T2} are mostly one-to-one connections, the dominant traffic in these clusters is http/https, ssh and peer to peer traffic. P2P traffic is defined as traffic where hosts uses both TCP and UDP ports concurrently for communication, They also choose arbitary ports for communication. Further analysis, reveals that clusters are split based on packet size and flow size.
While T1 has long living flows with large average packet sizes. T2 has large number of flows which are short lived and an average packet size less than T1.

\textbf{Clusters C1-C2} are mostly clients which connect to different servers of which C2 is dominated by DNS traffic and C1 comprises of both Web and Mail traffic.

\textbf{Clusters S1} are mostly clients which connect to different servers of which C2 is dominated by DNS traffic and C1 comprises of both Web and Mail traffic.

The above description clearly points out that host based behavior extraction produces a richer and finer classification of host behaviors than a port based classifier. For example, Http traffic is split across different clusters T or S which represents how the same protocol can be used differently in real time scenarios. Same is the case with SSH traffic also. While most of these clusters contain hosts using different protocols. They are similar in the sense that each cluster exhibits different functional behavior such as Server, Client or one-to-one traffic.

The \figref{behaviorsdec12} is a comparison between the classification made by a traditional port based classifer(left) and our host based classifier(right).

\begin{figure}[t]
	\centerline{\includegraphics[trim=2cm 2cm 2cm 2cm, scale = 0.5]{dec12-port-behaviors.pdf}}
	\caption{Comparison of hosts using port based classifier (left) and host behavior extractor (right) on December 12, 2017.}%
	\figlabel{behaviorsdec12}
\end{figure}


\section{Cross Validation}
We have compared our approach with other classification techniques to better understand the significance of our approach.

\textbf{\textit{Cross Validation with a port based classifier}}, We used a classic port based classifying technique to cross validate our results. Though, it isn't a perfect measure and fails in many cases it is good enough to classify a host to a particular class in most of the cases.
In the scenarios when this technique cannot classify a host to particular class we label it as a 'Mix' traffic. The heuristic that we used for labeling as 'Mix' traffic is when the dominant class accounts for less than 50 percent of the traffic by that host. Applying this procedure we observed 20 different classes of traffic, out of which the most frequently observed are discussed here: HTTP, DNS, SSH, MAIL, TELNET, FTP, CHAT, P2P, SMTP, MIX.

The cross-validation between the port based approach and  our procedure is reported in \tabref{validation}. 

\begin{table}[t]
	\caption{Cross-valdation of the host behavior extraction with port based analysis.}%
	\centerline{\includegraphics[scale = 0.6]{validation.pdf}}	
	\tablabel{validation}
\end{table}

From the highlited parts in the table we can safely conclude that 
there we have an high match of host classification which reflects the competence of the proposed procedure. It is also clearly evident from here that the nature of clusters described earlier confirm with the results obtained with cross-validation.The cross-validation also suggests that all the hosts that fall into a particular class based on port based analysis are not necessarily in the same cluster. They are dispersed across the clusters which shows the necessity of a host based behavior extraction.

For example, clusters S1 clearly shows that the hosts are SSH servers and cluster S2 ?. The distinction of clients is evident from the fact that C2 cluster has majority of DNS traffic and C1 clusters have mail and web traffic going out of the firewall. T1 is mostly requests and responses between hosts spread across different applications. The T2 clusters Mix shows that the hosts in the cluster have high interactive traffic across applications without falling into any majority class. Finally, for clusters containing hosts associated with a high proportion of scanners.

and our observation is that the classes are quite stable over 
several weeks which can be seen in the graph \figref{constant}. 
which is a graph between the number of clusters formed over a timeperiod of 3 months that confirms the constant behavior.
We can still update the host behaviors every couple of months or as per the change in usage of applications.

We further validated by looking at the distribution of 

\section{Synthesized Data: description and clustering}


\begin{itemize}
\item We were able to extract different host behaviors of interest to network admin, here we are mentioning two of them, namely scanners and DNS Query Responses. The cluster that contains scanners have a high incoming traffic trying to infiltrate the
network by scanning for open ports. The amount of packets and bytes transferred by hosts in this cluster is unsubstantial. The behavior exhibited by the hosts of this
cluster can be used by the network admin to monitor the security of the network.
The cluster that contains DNS Query Responses has heavy traffic aiming at a single
port. Studying about this cluster behavior over time will give an edge in planning
bandwidth of the network. The remarkable feature of our system is that it has extracted
the above behaviors without any prior knowledge of the hosts in the system or the input Netflow data.
\item We have observed that over periods of short intervals of time(few weeks), the number
of clusters have remained fairly constant which indicates that there has been no
rapid change in the behaviors exhibited by the hosts. And our system performed
seemingly well by choosing a constant number of clusters at the stage of pattern
detection. Figure \figref{constant} represents how the number of clusters formed for a months
data are close to constant.

\begin{figure}[t]
	\centerline{\includegraphics{constant.png}}
	\caption{ Plot of number of clusters formed over a time period.}%
	\figlabel{constant}
\end{figure}

\item  By mapping cluster behaviors of a given day with a reference day we were also able
to compare host behaviors. These comparisons can help network admin in capacity
planning, threat analysis and other network monitoring activities. We built a tool to analyze these host behaviors. 
We provide network admin with an option to compare
behaviors on two days as shown in figure \figref{compare_days}


\end{itemize}

 

\begin{figure}[t]
	\centerline{\includegraphics[scale = 0.45]{tool_compare_days.png}}
	\caption{Compare Host Behaviors on two days.}%
	\figlabel{compare_days}
\end{figure} 

The figure \figref{compare_weeks} gives an insight of how hosts behavior is changing over a month.
In the graph each line represents different host behaviors. The X-axis and Y-axis
represent the date on which this host behavior is observed and the number of hosts
which exhibited this behavior respectively.

\begin{figure}[t]
	\centerline{\includegraphics[scale = 0.45]{tool_compare_week.png}}
	\caption{Compare Host Behaviors over a time period.}%
	\figlabel{compare_weeks}
\end{figure} 



\section{Evaluation}

\tabref{evaluation} shows the planned evaluations of our sytsem which will be included in the final thesis defense.

\begin{table}[b]
		\caption{Planned Evaluations}%
	\centerline{\includegraphics[scale = 0.6]{evaluation.png}}
	\tablabel{evaluation}
\end{table}


As Netflow data is unlabeled it gets tough to test few functionalities of the system
that we have built. Hence, in order to overcome this we want to insert some behaviors
into the NetFlow data(create simulated environment) and verify if our system is able to
detect them. Even with simulated environment one has to remember that we are not going
to provide any information pertaining to the behaviors of the hosts of the system or the
simulated data yet our system is expected to uncover them. In addition to this we will also
present set of results using real data.




The two main functionalities that we would like to test are :
\begin{itemize}
	
	\item Detecting the anomalous hosts in the network through host behaviors and find other
	such hosts that are of interest to security of network.
	
	\item Helping network administrator to efficiently predict the capacity needs of the network.
\end{itemize}

We would like to create a micro benchmark using the existing NetFlow analysis tools
for both the above scenarios. For the first scenario, we synthesize flow data that has
scanners trying to enter the system along with the web traffic(Http requests, DNS requests
and others..) and pass it through the existing tools. We expect these tools to give a detailed
sketch about the traffic based on applications and ports. But, when the same traffic is
passed through our system the expected output is at least two clusters one with normal
web traffic and the other with scanners. This is the important point that we would like to
mention about our system that is it extracts the information that we have never asked it to
look for.
Before talking about the second scenario, let us see how the state-of-art network analysis
tools perform bandwidth management. Solarwinds is an organization that provides
Network Management softwares. One of it’s licensed products is ”Netflow Traffic Analyzer
and Bandwidth monitoring software”. This tool works with Cisco NetFlow, Juniper
J-Flow, sFlow, Huawei NetStream, and IPFIX flow data and monitors bandwidth use by
application, protocol, and IP address group and identifies which applications, and protocols
are consuming the most bandwidth. So,our benchmark for second scenario would be
to synthesize flow data which needs more than an aggregation on application or protocol
to determine capacity needed for network and pass it through open source tools that use
similar bandwidth management techniques as Solarwinds. Similarly, on the other hand
we pass this data through our system and our expectation would be that we will be find
few clusters that will help admin understand which hosts are consuming the network
bandwidth and take suitable actions.